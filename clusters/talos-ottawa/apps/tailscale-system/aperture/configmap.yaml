---
apiVersion: v1
kind: ConfigMap
metadata:
  name: aperture-config
  namespace: tailscale-system
data:
  config.yaml: |
    # Aperture LLM proxy configuration
    # Proxies requests to upstream ai.coder aperture instance over tailnet

    providers:
      # Upstream aperture instance with all LLM providers configured
      upstream:
        baseurl: "http://ai.coder:80"
        tailnet: true
        name: "Upstream Aperture"
        description: "Proxied LLM providers via ai.coder"
        models:
          # Anthropic
          - claude-haiku-4-5
          - claude-haiku-4-5-20251001
          - claude-opus-4-1
          - claude-opus-4-5
          - claude-opus-4-5-20251101
          - claude-sonnet-4-5
          - claude-sonnet-4-5-20250929
          # OpenAI
          - gpt-4.1
          - gpt-4.1-mini
          - gpt-4.1-nano
          - gpt-4o-mini
          - gpt-5
          - gpt-5-codex
          - gpt-5-mini
          - gpt-5-nano
          - gpt-5.1-codex
          - gpt-5.1-codex-mini
          # OpenRouter
          - deepseek/deepseek-chat-v3-0324
          - google/gemma-3-27b-it
          - meta-llama/llama-3.1-8b-instruct
          - minimax/minimax-m2
          - moonshotai/kimi-k2-thinking
          - openai/gpt-oss-120b
          - openai/gpt-oss-20b
          - qwen/qwen3-235b-a22b-2507
          - x-ai/grok-code-fast-1
          - z-ai/glm-4.6
        compatibility:
          openai_chat: true
          openai_responses: true
          anthropic_messages: true

    # MCP servers
    mcp_servers:
      - url: "http://usez-mcp.tailscale-system/mcp"
        tailnet: false

    # Database settings
    database:
      save_raws: false
      keep_encrypted_blobs: false
